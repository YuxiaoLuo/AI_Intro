{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ecb6e9a8",
   "metadata": {},
   "source": [
    "# Text Mining 1\n",
    "\n",
    "Instructor: [Yuxiao (Rain) Luo](https://yuxiaoluo.github.io)\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/YuxiaoLuo/AI_Intro/blob/main/week10_ImageMining_1.ipynb)\n",
    "\n",
    "## Keyword extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cf09d454",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Microsoft Windows [Version 10.0.26100.3476]\n",
      "(c) Microsoft Corporation. All rights reserved.\n",
      "\n",
      "C:\\Users\\YuxiaoLuo\\Documents\\python3\\AI_Intro>pip install rake-nltk\n",
      "Collecting rake-nltk\n",
      "  Downloading rake_nltk-1.0.6-py3-none-any.whl.metadata (6.4 kB)\n",
      "Collecting nltk<4.0.0,>=3.6.2 (from rake-nltk)\n",
      "  Downloading nltk-3.9.1-py3-none-any.whl.metadata (2.9 kB)\n",
      "Requirement already satisfied: click in c:\\users\\yuxiaoluo\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from nltk<4.0.0,>=3.6.2->rake-nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in c:\\users\\yuxiaoluo\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from nltk<4.0.0,>=3.6.2->rake-nltk) (1.4.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\yuxiaoluo\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from nltk<4.0.0,>=3.6.2->rake-nltk) (2024.11.6)\n",
      "Requirement already satisfied: tqdm in c:\\users\\yuxiaoluo\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from nltk<4.0.0,>=3.6.2->rake-nltk) (4.66.5)\n",
      "Requirement already satisfied: colorama in c:\\users\\yuxiaoluo\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from click->nltk<4.0.0,>=3.6.2->rake-nltk) (0.4.6)\n",
      "Downloading rake_nltk-1.0.6-py3-none-any.whl (9.1 kB)\n",
      "Downloading nltk-3.9.1-py3-none-any.whl (1.5 MB)\n",
      "   ---------------------------------------- 1.5/1.5 MB 26.5 MB/s eta 0:00:00\n",
      "Installing collected packages: nltk, rake-nltk\n",
      "Successfully installed nltk-3.9.1 rake-nltk-1.0.6\n",
      "\n",
      "C:\\Users\\YuxiaoLuo\\Documents\\python3\\AI_Intro>"
     ]
    }
   ],
   "source": [
    "%%cmd \n",
    "pip install rake-nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "36e6c40d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\YuxiaoLuo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\YuxiaoLuo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt_tab.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a70b2ad8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted Keywords and their scores:\n",
      "leading ai textbooks define: 14.5\n",
      "intelligent agents \":: 9.0\n",
      "natural intelligence displayed: 8.25\n",
      "intelligence demonstrated: 4.25\n",
      "artificial intelligence: 4.25\n",
      "artificial intelligence: 4.25\n",
      "takes actions: 4.0\n",
      "successfully achieving: 4.0\n",
      "solving .\": 4.0\n",
      "often used: 4.0\n",
      "human mind: 4.0\n",
      "humans associate: 3.5\n",
      "describe machines: 3.5\n",
      "ai: 2.5\n",
      "machines: 1.5\n",
      "humans: 1.5\n",
      "term: 1.0\n",
      "study: 1.0\n",
      "problem: 1.0\n",
      "perceives: 1.0\n",
      "mimic: 1.0\n",
      "maximize: 1.0\n",
      "learning: 1.0\n",
      "goals: 1.0\n",
      "functions: 1.0\n",
      "field: 1.0\n",
      "environment: 1.0\n",
      "device: 1.0\n",
      "contrast: 1.0\n",
      "computers: 1.0\n",
      "colloquially: 1.0\n",
      "cognitive: 1.0\n",
      "chance: 1.0\n"
     ]
    }
   ],
   "source": [
    "from rake_nltk import Rake\n",
    "\n",
    "# Sample text\n",
    "text = \"\"\"\n",
    "Artificial intelligence (AI) is intelligence demonstrated by machines, in contrast to the natural intelligence displayed by humans. \n",
    "Leading AI textbooks define the field as the study of \"intelligent agents\": any device that perceives its environment and takes actions \n",
    "that maximize its chance of successfully achieving its goals. Colloquially, the term \"artificial intelligence\" is often used to describe \n",
    "machines (or computers) that mimic \"cognitive\" functions that humans associate with the human mind, such as \"learning\" and \"problem-solving.\"\n",
    "\"\"\"\n",
    "\n",
    "# Initialize RAKE by providing stopwords\n",
    "rake = Rake()\n",
    "\n",
    "# Extract keywords by applying RAKE to the text\n",
    "rake.extract_keywords_from_text(text)\n",
    "\n",
    "# Get the ranked phrases and scores\n",
    "keywords = rake.get_ranked_phrases_with_scores()\n",
    "\n",
    "# Display the extracted keywords\n",
    "print(\"Extracted Keywords and their scores:\")\n",
    "for score, keyword in keywords:\n",
    "    print(f\"{keyword}: {score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77757481",
   "metadata": {},
   "source": [
    "## Emotion detection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "12b286f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4878f2d659644586baa106d31906a6f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.00k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\YuxiaoLuo\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\huggingface_hub\\file_download.py:159: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\YuxiaoLuo\\.cache\\huggingface\\hub\\models--j-hartmann--emotion-english-distilroberta-base. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36982bd78e5d4aaeb2730ab5ace45150",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/329M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b13da93e4b341ad858aa3043e4bfb62",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/294 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4809902e01424e05a56f76cca1f54639",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/798k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4319dc72ef0493fad9b2ac3ef985de4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c45a2d90f020499b8c8e2ddc06bb9c3d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/329M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "202c752320394bd8805fc941195eb250",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5bcf2d8a32134d6f867e65e5d2f0f858",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/239 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: I'm so happy with how things are going!\n",
      "Emotion: {'label': 'joy', 'score': 0.9858964085578918}\n",
      "\n",
      "Text: I'm feeling really down and upset.\n",
      "Emotion: {'label': 'sadness', 'score': 0.6187205910682678}\n",
      "\n",
      "Text: This is incredibly frustrating.\n",
      "Emotion: {'label': 'anger', 'score': 0.31297066807746887}\n",
      "\n",
      "Text: I just want to relax and have fun.\n",
      "Emotion: {'label': 'joy', 'score': 0.811194658279419}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Load the emotion detection model\n",
    "emotion_classifier = pipeline(\"text-classification\", model=\"j-hartmann/emotion-english-distilroberta-base\", framework=\"pt\")\n",
    "\n",
    "# Example texts to test for emotion detection\n",
    "texts = [\n",
    "    \"I'm so happy with how things are going!\",\n",
    "    \"I'm feeling really down and upset.\",\n",
    "    \"This is incredibly frustrating.\",\n",
    "    \"I just want to relax and have fun.\"\n",
    "]\n",
    "\n",
    "# Perform emotion detection on the sample texts\n",
    "emotion_results = emotion_classifier(texts)\n",
    "\n",
    "# Print the results\n",
    "for text, result in zip(texts, emotion_results):\n",
    "    print(f\"Text: {text}\\nEmotion: {result}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6e931c3",
   "metadata": {},
   "source": [
    "## Toxicity classification\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f9e928d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "032c9be090904d0d858457118bb971de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/811 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\YuxiaoLuo\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\huggingface_hub\\file_download.py:159: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\YuxiaoLuo\\.cache\\huggingface\\hub\\models--unitary--toxic-bert. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d0b264db22044348c6bf8c8dc6e1e08",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/438M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2fbcbcec648b49728da7177ff1efa3d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/174 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ab341787df04483a2df80b1eb5b437b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d82daab5b5247dd9ed9414afd6fbfa0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: I love the way you contribute to the team, keep up the great work!\n",
      "Toxicity: {'label': 'toxic', 'score': 0.0005700529436580837}\n",
      "\n",
      "Text: You are the worst person ever, I hate everything about you!\n",
      "Toxicity: {'label': 'toxic', 'score': 0.9779566526412964}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Load the toxicity classification model\n",
    "toxicity_classifier = pipeline(\"text-classification\", model=\"unitary/toxic-bert\", framework=\"pt\")\n",
    "\n",
    "# Example texts to test for toxicity\n",
    "texts = [\n",
    "    \"I love the way you contribute to the team, keep up the great work!\",\n",
    "    \"You are the worst person ever, I hate everything about you!\",\n",
    "]\n",
    "\n",
    "# Perform toxicity recognition on the sample texts\n",
    "toxicity_results = toxicity_classifier(texts)\n",
    "\n",
    "# Print the results\n",
    "for text, result in zip(texts, toxicity_results):\n",
    "    print(f\"Text: {text}\\nToxicity: {result}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93252fca",
   "metadata": {},
   "source": [
    "## Topic Modeling\n",
    "- A little bit challenging concept, so you should study to understand it\n",
    "- run the code block by block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "40375641",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Microsoft Windows [Version 10.0.26100.3476]\n",
      "(c) Microsoft Corporation. All rights reserved.\n",
      "\n",
      "C:\\Users\\YuxiaoLuo\\Documents\\python3\\AI_Intro>pip install gensim\n",
      "Collecting gensim\n",
      "  Downloading gensim-4.3.3-cp311-cp311-win_amd64.whl.metadata (8.2 kB)\n",
      "Requirement already satisfied: numpy<2.0,>=1.18.5 in c:\\users\\yuxiaoluo\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from gensim) (1.24.1)\n",
      "Requirement already satisfied: scipy<1.14.0,>=1.7.0 in c:\\users\\yuxiaoluo\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from gensim) (1.13.0)\n",
      "Collecting smart-open>=1.8.1 (from gensim)\n",
      "  Downloading smart_open-7.1.0-py3-none-any.whl.metadata (24 kB)\n",
      "Requirement already satisfied: wrapt in c:\\users\\yuxiaoluo\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from smart-open>=1.8.1->gensim) (1.16.0)\n",
      "Downloading gensim-4.3.3-cp311-cp311-win_amd64.whl (24.0 MB)\n",
      "   ---------------------------------------- 24.0/24.0 MB 36.1 MB/s eta 0:00:00\n",
      "Downloading smart_open-7.1.0-py3-none-any.whl (61 kB)\n",
      "Installing collected packages: smart-open, gensim\n",
      "Successfully installed gensim-4.3.3 smart-open-7.1.0\n",
      "\n",
      "C:\\Users\\YuxiaoLuo\\Documents\\python3\\AI_Intro>"
     ]
    }
   ],
   "source": [
    "%%cmd\n",
    "pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "41dfa051",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\YuxiaoLuo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c23bbfd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0,\n",
      "  '0.054*\"football\" + 0.054*\"language\" + 0.054*\"intelligence\" + '\n",
      "  '0.054*\"learning\" + 0.054*\"programming\" + 0.054*\"python\" + 0.054*\"data\" + '\n",
      "  '0.054*\"topics\" + 0.054*\"analysis\" + 0.054*\"love\"'),\n",
      " (1,\n",
      "  '0.038*\"using\" + 0.038*\"football\" + 0.038*\"language\" + 0.038*\"friends\" + '\n",
      "  '0.038*\"project\" + 0.038*\"fascinating\" + 0.038*\"machine\" + 0.038*\"working\" + '\n",
      "  '0.038*\"artificial\" + 0.038*\"exciting\"'),\n",
      " (2,\n",
      "  '0.065*\"processing\" + 0.065*\"working\" + 0.065*\"match\" + 0.065*\"techniques\" + '\n",
      "  '0.065*\"natural\" + 0.065*\"project\" + 0.065*\"night\" + 0.065*\"exciting\" + '\n",
      "  '0.065*\"last\" + 0.065*\"using\"')]\n",
      "\n",
      "Topic distribution for the new document: [(0, 0.49995598), (1, 0.019026836), (2, 0.48101723)]\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.models import LdaModel\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from pprint import pprint\n",
    "\n",
    "# Sample documents\n",
    "documents = [\n",
    "    \"I love playing football with my friends.\",\n",
    "    \"Python is my favorite programming language for data analysis.\",\n",
    "    \"Artificial Intelligence and machine learning are fascinating topics.\",\n",
    "    \"The football match last night was exciting.\",\n",
    "    \"I am working on a project using natural language processing techniques.\",\n",
    "]\n",
    "\n",
    "# Preprocess the documents (tokenization, stopwords removal)\n",
    "stop_words = set(stopwords.words('english'))\n",
    "texts = [\n",
    "    [word for word in word_tokenize(doc.lower()) if word.isalnum() and word not in stop_words]\n",
    "    for doc in documents\n",
    "]\n",
    "\n",
    "# Create a dictionary and a bag-of-words corpus\n",
    "dictionary = corpora.Dictionary(texts)\n",
    "corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "\n",
    "# Build the LDA model\n",
    "lda_model = LdaModel(corpus=corpus, id2word=dictionary, num_topics=3, random_state=100, update_every=1, passes=10, alpha='auto')\n",
    "\n",
    "# Print the topics\n",
    "pprint(lda_model.print_topics())\n",
    "\n",
    "# Example: Topic distribution for a new document\n",
    "new_doc = \"I enjoy coding in Python and working on AI projects.\"\n",
    "new_doc_bow = dictionary.doc2bow([word for word in word_tokenize(new_doc.lower()) if word.isalnum() and word not in stop_words])\n",
    "topic_distribution = lda_model.get_document_topics(new_doc_bow)\n",
    "print(f\"\\nTopic distribution for the new document: {topic_distribution}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2bfe1ea",
   "metadata": {},
   "source": [
    "## Named Entity Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bd33a34a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ca87598d4bc42c29e8ab79e2c91a805",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/998 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\YuxiaoLuo\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\huggingface_hub\\file_download.py:159: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\YuxiaoLuo\\.cache\\huggingface\\hub\\models--dbmdz--bert-large-cased-finetuned-conll03-english. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ecd4876f3ad941968cc41d79e02918bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.33G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at dbmdz/bert-large-cased-finetuned-conll03-english were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9f7f14c686d4454bfcdb145d521928a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/60.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5fe0f4683e24d00a666e4aba6a14af6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/213k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n",
      "C:\\Users\\YuxiaoLuo\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\pipelines\\token_classification.py:170: UserWarning: `grouped_entities` is deprecated and will be removed in version v5.0.0, defaulted to `aggregation_strategy=\"AggregationStrategy.SIMPLE\"` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Named Entities and their labels:\n",
      "Entity: Apple Inc, Label: ORG\n",
      "Entity: American, Label: MISC\n",
      "Entity: Cupertino, Label: LOC\n",
      "Entity: California, Label: LOC\n",
      "Entity: iPhone, Label: MISC\n",
      "Entity: iPad, Label: MISC\n",
      "Entity: Mac, Label: MISC\n",
      "Entity: Tim Cook, Label: PER\n",
      "Entity: Apple, Label: ORG\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Load the NER model\n",
    "ner_model = pipeline(\"ner\", model=\"dbmdz/bert-large-cased-finetuned-conll03-english\", grouped_entities=True)\n",
    "\n",
    "# Sample text\n",
    "text = \"\"\"\n",
    "Apple Inc. is an American multinational technology company headquartered in Cupertino, California, that designs, develops, and sells consumer electronics. \n",
    "Its most popular products include the iPhone, iPad, and Mac computers. Tim Cook is currently the CEO of Apple.\n",
    "\"\"\"\n",
    "\n",
    "# Perform NER\n",
    "ner_results = ner_model(text)\n",
    "\n",
    "# Display the named entities with their labels\n",
    "print(\"Named Entities and their labels:\")\n",
    "for entity in ner_results:\n",
    "    print(f\"Entity: {entity['word']}, Label: {entity['entity_group']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59f86d4f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
